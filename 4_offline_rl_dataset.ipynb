{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a1ee793-80d5-4f08-b905-2cc458be2654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files here: ['.ipynb_checkpoints', 'accepted_2007_to_2018Q4.csv.gz', 'dl_predictions.csv', 'final_dl_model.pth', 'final_predictions.csv', 'final_preprocessor.pkl', 'final_preprocessor_fitted.pkl', 'final_xgb_model.pkl', 'NEW_1_Preprocessing.ipynb', 'OFFLINE RL ENVIRONMENT.ipynb', 'offline_rl_dataset.ipynb', 'offline_rl_dataset.npz', 'sample_200k.csv', 'xgb_model.pkl', '_Deep_Learning_Model.ipynb']\n",
      "Loaded npz shapes: states (200000, 149) actions (200000,)\n"
     ]
    }
   ],
   "source": [
    "import os, numpy as np, pandas as pd\n",
    "\n",
    "print(\"Files here:\", os.listdir())\n",
    "\n",
    "data = np.load(\"offline_rl_dataset.npz\")\n",
    "states = data[\"states\"]\n",
    "actions = data[\"actions\"]\n",
    "next_states = data[\"next_states\"]\n",
    "dones = data[\"dones\"]\n",
    "print(\"Loaded npz shapes: states\", states.shape, \"actions\", actions.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1093112-6969-4b34-ac97-d030a2d5a732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded df shape: (200000, 151)\n",
      "Columns (first 25): ['id', 'member_id', 'loan_amnt', 'funded_amnt', 'funded_amnt_inv', 'term', 'int_rate', 'installment', 'grade', 'sub_grade', 'emp_title', 'emp_length', 'home_ownership', 'annual_inc', 'verification_status', 'issue_d', 'loan_status', 'pymnt_plan', 'url', 'desc', 'purpose', 'title', 'zip_code', 'addr_state', 'dti']\n"
     ]
    }
   ],
   "source": [
    "# load CSV (low_memory=False to avoid DtypeWarning)\n",
    "df = pd.read_csv(\"sample_200k.csv\", low_memory=False)\n",
    "print(\"Loaded df shape:\", df.shape)\n",
    "print(\"Columns (first 25):\", list(df.columns)[:25])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a27b96f4-5bef-4f34-88a6-5aa6990db72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target distribution (counts):\n",
      " target\n",
      "0    173134\n",
      "1     26866\n",
      "Name: count, dtype: int64\n",
      "loan_amnt: min/max 0.0 40000.0\n",
      "int_rate: min/max 0.0 30.99\n"
     ]
    }
   ],
   "source": [
    "# create binary target if missing\n",
    "if 'target' not in df.columns:\n",
    "    df['target'] = df['loan_status'].apply(lambda x: 1 if str(x) in [\"Charged Off\", \"Default\"] else 0)\n",
    "\n",
    "# convert numeric columns safely and fill missing values\n",
    "df['loan_amnt'] = pd.to_numeric(df['loan_amnt'], errors='coerce').fillna(0.0)\n",
    "df['int_rate']  = pd.to_numeric(df['int_rate'], errors='coerce').fillna(0.0)\n",
    "\n",
    "print(\"target distribution (counts):\\n\", df['target'].value_counts())\n",
    "print(\"loan_amnt: min/max\", df['loan_amnt'].min(), df['loan_amnt'].max())\n",
    "print(\"int_rate: min/max\", df['int_rate'].min(), df['int_rate'].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8efbd41-e300-4690-8dd3-61ce877931ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected int_rate likely in percent (e.g. 13.5). Converting by /100.\n",
      "rewards shape: (200000,) min/max: -40000.0 12336.0\n"
     ]
    }
   ],
   "source": [
    "if df['int_rate'].max() > 1.0:\n",
    "    print(\"Detected int_rate likely in percent (e.g. 13.5). Converting by /100.\")\n",
    "    df['int_rate'] = df['int_rate'] / 100.0\n",
    "\n",
    "df['profit_if_paid'] = df['loan_amnt'] * df['int_rate']\n",
    "\n",
    "df['reward'] = df.apply(lambda r: -r['loan_amnt'] if r['target']==1 else r['profit_if_paid'], axis=1)\n",
    "\n",
    "rewards = df['reward'].fillna(0.0).astype(float).values\n",
    "rewards = np.nan_to_num(rewards, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "print(\"rewards shape:\", rewards.shape, \"min/max:\", rewards.min(), rewards.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f1a9a3a-7a83-4b28-8f45-c06983cc7ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengths match. Good to proceed.\n"
     ]
    }
   ],
   "source": [
    "if len(rewards) != states.shape[0]:\n",
    "    print(\"WARNING: rewards length != states rows:\", len(rewards), \"vs\", states.shape[0])\n",
    "    print(\"If dataset rows are out of sync you must align them by a unique id. Do you have a unique 'id' column? ->\", 'id' in df.columns)\n",
    "else:\n",
    "    print(\"Lengths match. Good to proceed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e168df1-5dd9-4b2b-83f6-82c795282601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward mean/std: -441.4055658500001 6939.831579618871\n",
      "Normalized: mean, std -> 1.2647660696529783e-17 0.999999999855904\n",
      "NaNs in normalized: 0\n"
     ]
    }
   ],
   "source": [
    "reward_mean = rewards.mean()\n",
    "reward_std  = rewards.std() + 1e-6\n",
    "rewards_norm = (rewards - reward_mean) / reward_std\n",
    "\n",
    "print(\"Reward mean/std:\", reward_mean, reward_std)\n",
    "print(\"Normalized: mean, std ->\", rewards_norm.mean(), rewards_norm.std())\n",
    "print(\"NaNs in normalized:\", np.isnan(rewards_norm).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "04e6ec0c-d9a4-464c-9f15-b1f453bba9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved offline_rl_dataset_fixed.npz\n"
     ]
    }
   ],
   "source": [
    "np.savez(\"offline_rl_dataset_fixed.npz\",\n",
    "         states=states,\n",
    "         actions=actions,\n",
    "         rewards=rewards_norm,      # normalized rewards\n",
    "         next_states=next_states,\n",
    "         dones=dones)\n",
    "\n",
    "print(\"Saved offline_rl_dataset_fixed.npz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ae88bb0-7c95-4b23-9063-4326ac494224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded FIXED dataset: (200000, 149) (200000,)\n",
      "Reward stats: mean= 1.2647660696529783e-17 std= 0.999999999855904\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "data = np.load(\"offline_rl_dataset_fixed.npz\")\n",
    "\n",
    "states = data[\"states\"]\n",
    "actions = data[\"actions\"]\n",
    "rewards = data[\"rewards\"]\n",
    "next_states = data[\"next_states\"]\n",
    "dones = data[\"dones\"]\n",
    "\n",
    "print(\"Loaded FIXED dataset:\", states.shape, rewards.shape)\n",
    "print(\"Reward stats: mean=\", rewards.mean(), \"std=\", rewards.std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2e3d765b-afcf-45b1-baa8-77fcff17e806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RL Model Ready\n"
     ]
    }
   ],
   "source": [
    "state_dim = states.shape[1]\n",
    "action_dim = 2  # {0: deny, 1: approve}\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "q_net = QNetwork(state_dim, action_dim)\n",
    "optimizer = optim.Adam(q_net.parameters(), lr=1e-3)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "print(\"RL Model Ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2ad25c99-bac7-439e-b20b-c2a42df47b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting RL Training...\n",
      "Epoch 1/6 - Loss: 11587191.4756\n",
      "Epoch 2/6 - Loss: 1588.1930\n",
      "Epoch 3/6 - Loss: 958.5764\n",
      "Epoch 4/6 - Loss: 793.0731\n",
      "Epoch 5/6 - Loss: 246085.1384\n",
      "Epoch 6/6 - Loss: 429653.0741\n"
     ]
    }
   ],
   "source": [
    "# Convert dataset to torch\n",
    "s = torch.tensor(states, dtype=torch.float32)\n",
    "a = torch.tensor(actions, dtype=torch.long)\n",
    "r = torch.tensor(rewards, dtype=torch.float32)\n",
    "ns = torch.tensor(next_states, dtype=torch.float32)\n",
    "d = torch.tensor(dones, dtype=torch.float32)\n",
    "\n",
    "gamma = 0.99\n",
    "epochs = 6\n",
    "batch_size = 512\n",
    "\n",
    "print(\"Starting RL Training...\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # shuffle indices\n",
    "    idx = torch.randperm(len(s))\n",
    "\n",
    "    total_loss = 0\n",
    "    for i in range(0, len(s), batch_size):\n",
    "        batch = idx[i:i+batch_size]\n",
    "\n",
    "        s_b = s[batch]\n",
    "        a_b = a[batch]\n",
    "        r_b = r[batch]\n",
    "        ns_b = ns[batch]\n",
    "        d_b = d[batch]\n",
    "\n",
    "        # Q(s, a)\n",
    "        q_values = q_net(s_b)\n",
    "        q_sa = q_values.gather(1, a_b.unsqueeze(1)).squeeze()\n",
    "\n",
    "        # target: r + gamma * max(Q(s', a')) * (1-done)\n",
    "        with torch.no_grad():\n",
    "            next_q = q_net(ns_b).max(1)[0]\n",
    "            target = r_b + gamma * next_q * (1 - d_b)\n",
    "\n",
    "        loss = loss_fn(q_sa, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d30c6d23-9642-4207-b5a2-d65666c7e5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Policy Value of RL Agent: -4.565835667760856e-06\n"
     ]
    }
   ],
   "source": [
    "q_net.eval()\n",
    "with torch.no_grad():\n",
    "    qs = q_net(torch.tensor(states, dtype=torch.float32))\n",
    "    policy_actions = qs.argmax(1).numpy()\n",
    "\n",
    "# estimated policy value = average reward of the actions agent chooses\n",
    "selected_rewards = rewards[policy_actions == 1]\n",
    "\n",
    "if len(selected_rewards) > 0:\n",
    "    rl_value = selected_rewards.mean()\n",
    "else:\n",
    "    rl_value = 0\n",
    "\n",
    "print(\"Estimated Policy Value of RL Agent:\", rl_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ccf628-262c-458d-adad-64e383a04877",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

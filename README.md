ğŸš€ Loan Decision Optimization: Deep Learning + Offline Reinforcement Learning
ğŸ“Œ Project Overview

This project builds an intelligent loan approval system using:

Supervised Deep Learning (risk prediction)

Offline Reinforcement Learning (profit-maximizing policy)

Dataset: LendingClub Accepted Loans (2007â€“2018)
Goal: Maximize profitability while reducing loan defaults

âœ… Task 1 â€” EDA & Preprocessing

Notebook: 1_Preprocessing.ipynb

âœ” What Was Done

1ï¸âƒ£ Exploratory Data Analysis

Checked missing values

Studied distribution of key features

Identified target imbalance

Removed non-predictive columns (id, url, title, etc.)

2ï¸âƒ£ Feature Engineering

loan_to_income = loan_amnt / annual_inc

amt_per_term = loan_amnt / term

3ï¸âƒ£ Preprocessing Pipeline

SimpleImputer (median / most_frequent)

StandardScaler for numeric scaling

OrdinalEncoder for categorical encoding

Combined using ColumnTransformer

âœ” Output

final_preprocessor.pkl

Cleaned dataset for model training

ğŸ¤– Task 2 â€” Deep Learning Model (Supervised Learning)

Notebook: 2_Deep_Learning_Model.ipynb

ğŸ¯ Target Definition

0 â†’ Fully Paid

1 â†’ Default / Charged Off

ğŸ§± Model Architecture (PyTorch MLP)

148 â†’ 256 â†’ 128 â†’ 64 â†’ 1

Activations: ReLU

Regularization: Dropout

Loss: BCEWithLogitsLoss

Optimizer: Adam

ğŸ“Š Results

AUC â‰ˆ 0.99

Best F1-score â†’ tuned threshold

ğŸ” DL Policy
If predicted_default_probability < threshold:
    Approve Loan
Else:
    Deny Loan

ğŸ§  Task 3 â€” Offline Reinforcement Learning
Notebook: 3_RL_Environment.ipynb & 4_Offline_RL_Training.ipynb
ğŸ“Œ RL Setup

State (s): Preprocessed feature vector (149 values)
Action (a):

0 â†’ Deny

1 â†’ Approve

Reward (r):

Deny â†’ 0

Approve + Fully Paid â†’ loan_amnt * int_rate

Approve + Default â†’ -loan_amnt

âœ” RL Dataset Created

Saved as:

offline_rl_dataset.npz

offline_rl_dataset_fixed.npz

Contains:

states

actions

rewards

next_states

dones

ğŸ‹ï¸ RL Training

Offline Q-learning (no environment interaction)

Q-network learns:

Approve if Q(s,1) > Q(s,0)

ğŸ“ˆ RL Output

Learned approval policy

Estimated policy value (expected profit of RL decisions)

ğŸ“Š Task 4 â€” Analysis & Business Insights
1ï¸âƒ£ Why DL Metrics (AUC & F1)?

AUC â†’ how well the model separates good borrowers vs risky borrowers

F1 â†’ best balance between identifying defaults & minimizing false approvals

Helps as risk classifier

2ï¸âƒ£ Why RL Metric = Policy Value?

Measures profit, not accuracy

Answers business question:
â€œHow much money will this approval policy make?â€

3ï¸âƒ£ DL vs RL Decisions

DL denies high-risk applicants

RL approves some high-risk applicants if expected interest > expected loss

RL focuses on maximizing money, not accuracy

4ï¸âƒ£ Future Improvements

Use advanced Offline RL algorithms (CQL, IQL, BCQ)

Add financial & behavioral data

Improve reward design

Create simulated environment for real-time RL

ğŸ§ª How to Run This Project
Install dependencies
pip install -r requirements.txt

Run notebooks in order:

1_Preprocessing.ipynb

2_Deep_Learning_Model.ipynb

3_RL_Environment.ipynb

4_Offline_RL_Training.ipynb

ğŸ“ Conclusion

This project builds two complementary systems:

Deep Learning Model â†’ Predicts default risk with high accuracy (AUC â‰ˆ 0.99)

Offline RL Agent â†’ Learns approval decisions that maximize expected financial return

Together, they create a smart and profitable loan approval strategy for fintech applications.
